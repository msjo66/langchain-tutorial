{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01장\n",
    "========\n",
    "\n",
    "## 들어가기\n",
    "\n",
    "* 2022년 10월에 Harrison Chase가 Langchain을 만들어서 commit을 했다(github에 push를 했다는 뜻인 듯).\n",
    "* Langchain이란 LLM[^1]을 core에 두고 그 주변에 여러가지 tool들을 제공을 해서 LLM을 이용한 애플리케이션을 쉽게 만들도록 해주는 framework[^2]이다.\n",
    "* Langchain을 이용해서 챗봇, GQA(Generative Question-Answering), 요약(summarization)등을 위한 애플리케이션을 만들 수 있다.\n",
    "* Langchain의 핵심 아이디어는 LLM을 이용해서 위에서 언급한 애플리케이션을 만들 때 재사용 가능한 컴포넌트들을 '연결(chain)'해서 [^3] [^4] 다양한 유스 케이스를 위한 애플리케이션을 만든다는 것이다.\n",
    "    * Langchain은 '컴포넌트'를 '연결(chain)'하는 것이다.\n",
    "    * Langchain에는(공식 문서에 의하면) 17개의 컴포넌트가 있다(Chat models, LLMs, Messages, Prompt templates, Example selectors, Output Parsers, Chat history, Documents, Document loaders, Text splitters, Embedding models, Vector stores, Retrievers, Tools, Toolkits, Agents, Callbacks).\n",
    "    * 대표적인 컴포넌트는 아래와 같다. [^5]\n",
    "        * <b>Prompt templates</b> : 말 그대로 재사용 가능하도록 프롬프트에 대한 템플릿을 만드는 것이다. 점점 알아가겠지만 LLM 애플리케이션의 핵심은 결국 Prompt를 어떻게 만드냐는 것이다. 여러가지 컴포넌트들은 실제로는 이 Prompt를 변경해서 LLM에게 답변을 하게 하는 것들인 경우가 많다.\n",
    "        * <b>LLMs</b> : GPT-3, BLOOM, Mistral 과 같은 LLM 모델을 말한다. 즉, 다양한 LLM들을 상황에 맞게 바꿔치기 할 수 있도록 추상화 해놓은 것이다.\n",
    "        * <b>Agents</b> : Agent는 LLMs을 사용해서 어떠한 action을 해야하는 지를 결정한다. 웹 검색, 계산기와 같은 툴을 사용하기도 한다.\n",
    "        * <b>Memory</b> : short-term과 long-term이 있다. LLMs 자체는 이전의 대화를 기억하지 못한다(물론 GPT-4o같은 것은 이제는 기억한다).\n",
    "\n",
    "[^1]: 저자만 그런지 모르겠지만, 처음에는 LLM이 뭔지 막연했다. 이를 위해서 DeepLearning 관련한 여러가지 문서를 읽어가면서 NLP(National Language Procecssing), Tokenizer, BoW(Bag of Words), Vector, Similarity, Attention, Transformer.. 등등 많은 단어를 이해하려고 노력했지만 그래서는 AI 애플리케이션을 언제 만들 수 있는 지 모르겠다는 느낌이 들었다. 우리가 Oracle DB를 사용할 때 내부 Block Structure를 알아야 사용할 수 있는가? Spring Boot 애플리케이션을 만들 때 JVM의 Garbage Collection을 알아야 하는가? 물론 만들다 보면 서서히 알게 되는 경우도 있고, 더 깊이 알기 위해서 필요할 때 공부할 수도 있다. 일단은 그런거 모른 채로 시작하자. 지금 이 순간에는 LLM을 그냥 구글 검색에서 나오는 대로 확률적 앵무새(stochastic parrot)라고 생각하면 된다. '확률적'이라는 말이 또 헷갈리게 하니까 그냥 앵무새라고 생각하자. 우리가 세상에 이런일이 같은 TV 프로그램에서 봐왔던 그 앵무새다. 말을 잘 하는 것 같지만 사실상 앵무새는 그 뜻을 모른다. 다만 주인이 말하는 단어들을 이전에 들어왔기 때문에 그런 단어가 나오면 그 때 그 다음에 했던 대답을 다시 하는 것이다. LLM은 뜻을 모른채 들어오는 문장에 대해서 자기가 배우왔던 기존의 문장에서 가장 적합한 단어들을 모아서 문장이 되도록 만들어서 뱉어내는 블랙박스라고 생각하자.\n",
    "\n",
    "[^2]: framework과 library의 차이도 상식차원에서 알아두면 좋다. framework은 말 그대로 '틀'이다. 코드를 짜는 형식이 있다는 뜻이다. 즉 애플리케이션을 만들 때 구조가 정해져 있다는 것이다. 그래서 그 구조를 알고 그에 맞게 코드를 짜야 한다. Spring MVC는 framework이지만 jdbc(구체적으로 jdbc 인터페이스와 각 DB별 구현체)는 library이다. Angular는 framework이지만 react는 library이다. 물론 framework은 그 안에서 구조에 맞도록 코드를 짜기 위해 필요한 library들을 스스로 만들어서 제공을 하고있다. langchain framework에도 자체적으로 제공하는 많은 library들이 있다. 일반적으로 framework은 그 구조와 그러한 구조를 만들어 낸 사상을 알아야하기 때문에 공부를 많이 해야 한다. 그래서 framework은 자유도가 많이 없고, library는 자유도가 있기 때문에 library만 가지고 만드는 것이 좀 더 유연하게 애플리케이션을 만들 수 있다. '유연하다'라는 단어가 좋아보이지만 꼭 그렇지는 않다. 특히 유지보수의 측면에서 그렇다.\n",
    "\n",
    "[^3]: Langchain은 framework의 역할을 제대로 하기 위해 기능과 아키텍처가 계속해서 발전해 온 것 같다. 그래서 현재로써는 아래 그림과 같이 아주 거대하다. 한 번에 모든 것을 알 수 없고 시간이 많이 걸린다. 기회가 되면 계속해서 설명해 보겠다. ![Langchain 아키텍처](https://python.langchain.com/v0.2/svg/langchain_stack.svg) \n",
    "\n",
    "[^4]: 원문을 읽어보면 컴포넌트, 라이브러리, 모듈 이라는 단어가 같이 나오는데 이를 구분하기가 쉽지 않다. 일단 저자의 해석에 의하면 컴포넌트와 모듈을 서로 섞어서 쓰고 있는 것 같다.\n",
    "\n",
    "[^5]: '대표적'것은 원문에는 'several'이라고 표현되어 있는 것이다. '대표적'이라는 뜻은 '초보자에게 가르치는 데 제일 많이 사용되는' 이라는 뜻과 비슷하다.\n",
    "\n",
    "## Prompt template과 LLM 부터 시작하기\n",
    "\n",
    "### 첫번째 Prompt Template\n",
    "\n",
    "먼저 명령 프롬프트에서 아래와 같이 package를 추가한다.\n",
    "\n",
    "```console\n",
    "pinecone-ai-handbook-m0t52s90-py3.10 ❯ poetry add langchain\n",
    "pinecone-ai-handbook-m0t52s90-py3.10 ❯ poetry add langchain-community\n",
    "pinecone-ai-handbook-m0t52s90-py3.10 ❯ poetry add huggingface_hub\n",
    "pinecone-ai-handbook-m0t52s90-py3.10 ❯ poetry add openai\n",
    "pinecone-ai-handbook-m0t52s90-py3.10 ❯ poetry add ipykernel\n",
    "pinecone-ai-handbook-m0t52s90-py3.10 ❯ poetry add python-dotenv\n",
    "```\n",
    "ipykernel과 python-dotenv는 langchain과는 직접적인 관련이 없는 package이다. ipykernel은 jupyter notebook 실행을 위한 것이고, python-dotenv는 환경변수 파일로부터 환경변수를 가져오는 패키지이다. 환경변수파일에는 openai를 api로 사용하기 위한 api key가 들어있다. api key는 소스로 배포되거나 공개가 되면 보안 위협이 있으므로 소스코드에 넣으면 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 PromptTemplate을 통해 prompt를 만든다. 코드는 아래와 같다. prompt template과 question을 통해서 우리가 만들게 되는 prompt는\n",
    "> 질문: 2019년 SKTelecom Open 골프대회 우승자가 누구야? 답변:\n",
    "이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template= \"\"\"\n",
    "질문: {question}\n",
    "\n",
    "답변: \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "question = \"2019년 SKTelecom Open 골프대회 우승자가 누구야?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제로 huggingface의 api를 호출해서 답변을 얻어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서부터 환경변수를 읽어온다. 필요한 환경변수는 OPENAI_API_KEY 이다.\n",
    "load_dotenv()\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xxl',\n",
    "    model_kwargs={'temperature':1e-10}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about NFL 2010\n",
    "print(llm_chain.run(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "골프에 관심이 있다면 위 답변이 틀렸다는 것을 알 수 있겠지만 그건 다음에 해결할 문제이다. 일단 우리는 코드를 짜서, LLM API를 이용해서 LLM으로부터 답을 이끌어냈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='')], [Generation(text='')], [Generation(text='')], [Generation(text='')]], llm_output=None, run=[RunInfo(run_id=UUID('180dc396-97ca-4ec3-b48a-4653d70d4863')), RunInfo(run_id=UUID('984f2c58-d6ca-4011-9350-272cdf2333f1')), RunInfo(run_id=UUID('921f77a4-0018-4131-abb8-e78275c80922')), RunInfo(run_id=UUID('f516dba7-d983-45df-a3b3-4171af5e44a6'))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"1998년 한국시리즈 우승팀은 어디야?\"},\n",
    "    {'question': \"내 키는 173 센티미터야. 몇 피트 몇 인치이지?\"},\n",
    "    {'question': \"달에 12번째로 가 본 사람은 누구야?\"},\n",
    "    {'question': \"거미은 눈이 몇 개지?\"}\n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"아래 질문들에 대해서 한 번에 하나씩 답변해야 한다.\n",
    "\n",
    "질문들:\n",
    "{questions}\n",
    "\n",
    "답변들:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"1998년 한국시리즈 우승팀은 어디야?\\n\" +\n",
    "    \"내 키는 173 센티미터야. 몇 피트 몇 인치이지?\\n\" +\n",
    "    \"달에 12번째로 가 본 사람은 누구야?\" +\n",
    "    \"거미은 눈이 몇 개지?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019년 SKTelecom Open 골프대회 우승자는 안병훈(한국)입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain, OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "template = \"\"\"질문: {question}\n",
    "\n",
    "답변: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "question=\"2019년 SKTelecom Open 골프대회 우승자가 누구야?\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone-ai-handbook-m0t52s90-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
